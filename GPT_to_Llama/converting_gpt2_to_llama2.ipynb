{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TX7zRW_3e06"
      },
      "source": [
        "# Converting a From-Scratch GPT Architecture to Llama 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCbJPAOu3e07"
      },
      "source": [
        "1. In this notebook, we convert the original GPT architecture into a Llama 2 model step by step (note the GPT and GPT-2 share the same architecture)\n",
        "2. For more information, please see the Llama 2 paper: [Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)](https://arxiv.org/pdf/2307.09288)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whqT2shh3e08"
      },
      "source": [
        "Packages that are being used in this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kz1OI5qB3e09",
        "outputId": "ab8c5918-8932-4d0d-f3d9-221751aef088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "huggingface_hub version: 0.30.1\n",
            "sentencepiece version: 0.2.0\n",
            "torch version: 2.6.0+cu124\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"huggingface_hub\",  # to download pretrained weights\n",
        "    \"sentencepiece\",    # to implement the tokenizer\n",
        "    \"torch\",            # to implement the model\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO_KkMrt3e0-"
      },
      "source": [
        "# 1. Convert the GPT model implementation step by step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNAW2ly_3e0-"
      },
      "source": [
        "## 1.1 Replace LayerNorm with RMSNorm layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UUdB0LU3e0_"
      },
      "source": [
        "- First, we replace LayerNorm by Root Mean Square Layer Normalization (RMSNorm)\n",
        "- LayerNorm normalizes inputs using mean and variance, while RMSNorm uses only the root mean square, which improves computational efficiency\n",
        "- The RMSNorm operation is as follows, where $x$ is the input $\\gamma$ is a trainable parameter (vector), and $\\epsilon$ is a small constant to avoid zero-division errors:\n",
        "\n",
        "$$y_i = \\frac{x_i}{\\text{RMS}(x)} \\gamma_i, \\quad \\text{where} \\quad \\text{RMS}(x) = \\sqrt{\\epsilon + \\frac{1}{n} \\sum x_i^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "sAihF-FU3e0_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "#####################################\n",
        "# GPT LayerNorm Function\n",
        "#####################################\n",
        "\n",
        "# class LayerNorm(nn.Module):\n",
        "#     def __init__(self, emb_dim):\n",
        "#         super().__init__()\n",
        "#         self.eps = 1e-5\n",
        "#         self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "#         self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         mean = x.mean(dim=-1, keepdim=True)\n",
        "#         var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "#         norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "#         return self.scale * norm_x + self.shift\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self,emb_dim,eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(self.emb_dim)).float()\n",
        "\n",
        "    def forward(self,x):\n",
        "        means = x.pow(2).mean(dim=-1, keepdim=True) # this calculates the summation inside sqrt.\n",
        "        x_normed = x * torch.rsqrt(means + self.eps) # using torch.rsqrt we calculated and reciprocal the inner value\n",
        "        return (x_normed * self.weight).to(dtype=x.dtype) # now we multiply weight vector with x / RMS(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyNTIniN3e1A"
      },
      "source": [
        "The following code cell checks that this implementation works the same as PyTorch's built-in implementation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2_KOvPT13e1A"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "example_batch = torch.randn(2, 3, 4)\n",
        "\n",
        "rms_norm = RMSNorm(emb_dim=example_batch.shape[-1])\n",
        "rmsnorm_pytorch = torch.nn.RMSNorm(example_batch.shape[-1], eps=1e-5)\n",
        "\n",
        "assert torch.allclose(rms_norm(example_batch), rmsnorm_pytorch(example_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q2OEBj03e1A"
      },
      "source": [
        "## 1.2 Replace GELU with SiLU activation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw2Dw1QL3e1B"
      },
      "source": [
        "- Llama uses the SiLU activation function (instead of GELU), which is also known as the Swish function:\n",
        "\n",
        "$$\n",
        "\\text{silu}(x) = x \\cdot \\sigma(x), \\quad \\text{where} \\quad \\sigma(x) \\text{ is the logistic sigmoid.}\n",
        "$$\n",
        "\n",
        "- For more information, see the SiLU paper: [Sigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning (2017)](https://arxiv.org/abs/1702.03118)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "gJMlvEn13e1B"
      },
      "outputs": [],
      "source": [
        "#####################################\n",
        "# GELU Function used in GPT model\n",
        "#####################################\n",
        "\n",
        "# class GELU(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return 0.5 * x * (1 + torch.tanh(\n",
        "#             torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "#             (x + 0.044715 * torch.pow(x, 3))\n",
        "#         ))\n",
        "\n",
        "\n",
        "class SiLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiLU,self).__init__()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return x * torch.sigmoid(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Q7cTrbW93e1B"
      },
      "outputs": [],
      "source": [
        "silu = SiLU()\n",
        "\n",
        "assert torch.allclose(silu(example_batch), torch.nn.functional.silu(example_batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrsBDZev3e1B"
      },
      "source": [
        "&nbsp;\n",
        "## 1.3 Update the FeedForward module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-iY9t5Q3e1B"
      },
      "source": [
        "- In fact, Llama uses a \"Gates Linear Unit\" (GLU) variant of SiLU called SwiGLU, which essentially results in a slightly differently structured `FeedForward` module\n",
        "- SwiGLU uses a gating mechanism in the feedforward layer, with the formula:\n",
        "\n",
        "$$\\text{SwiGLU}(x) = \\text{SiLU}(\\text{Linear}_1(x)) * (\\text{Linear}_2(x))$$\n",
        "\n",
        "- Here, $\\text{Linear}_1$ and $\\text{Linear}_2$ are two linear layers, and $*$ denotes element-wise multiplication\n",
        "- The third linear layer, $\\text{Linear}_3$, is applied after this gated activation\n",
        "\n",
        "- For more information, see SwiGLU paper: [GLU Variants Improve Transformer (2020)](https://arxiv.org/abs/2002.05202)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Z30gogG03e1C"
      },
      "outputs": [],
      "source": [
        "#####################################\n",
        "# FeedForward modue from GPT uses GELU as shown\n",
        "#####################################\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, cfg):\n",
        "#         super().__init__()\n",
        "#         self.layers = nn.Sequential(\n",
        "#             nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "#             GELU(),\n",
        "#             nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.layers(x)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"] ,cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        self.silu = SiLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_fc1 = self.fc1(x)\n",
        "        x_fc2 = self.fc2(x)\n",
        "        x = self.silu(x_fc1) * x_fc2  # silu of fc1 is multiplied by fc2 then passed through fc3 as shown below.\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cATPhyh13e1C"
      },
      "source": [
        "1. Note that we also added a dtype=cfg[\"dtype\"] setting above, which will allow us to load the model directly in lower precision formats later to reduce memory usage (versus instantiating it in the original 32-bit precision format and then converting it)\n",
        "2. We also set bias=False since Llama doesn't use any bias units"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovvo7AFo3e1C"
      },
      "source": [
        "&nbsp;\n",
        "## 1.4 Implement RoPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgRYqHUT3e1C"
      },
      "source": [
        "- In the GPT model, the positional embeddings are implemented as follows:\n",
        "\n",
        "```python\n",
        "self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "```\n",
        "\n",
        "- Unlike traditional absolute positional embeddings, Llama uses rotary position embeddings (RoPE), which enable it to capture both absolute and relative positional information simultaneously\n",
        "- The reference paper for RoPE is [RoFormer: Enhanced Transformer with Rotary Position Embedding (2021)](https://arxiv.org/abs/2104.09864)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jxqFcchq3e1C"
      },
      "outputs": [],
      "source": [
        "def precompute_rope_params(head_dim, theta_base=10_000, context_length=4096):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\"\n",
        "\n",
        "    # Compute the inverse frequencies\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    # Generate position indices\n",
        "    positions = torch.arange(context_length)\n",
        "\n",
        "    # Compute the angles\n",
        "    angles = positions[:, None] * inv_freq[None, :]  # Shape: (context_length, head_dim // 2)\n",
        "\n",
        "    # Expand angles to match the head_dim\n",
        "    angles = torch.cat([angles, angles], dim=1)  # Shape: (context_length, head_dim)\n",
        "\n",
        "    # Precompute sine and cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin\n",
        "\n",
        "def compute_rope(x, cos, sin):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\"\n",
        "\n",
        "    # Split x into first half and second half\n",
        "    x1 = x[..., : head_dim // 2]  # First half\n",
        "    x2 = x[..., head_dim // 2 :]  # Second half\n",
        "\n",
        "    # Adjust sin and cos shapes\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)  # Shape: (1, 1, seq_len, head_dim)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # Apply the rotary transformation\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq-0ufID3e1D"
      },
      "source": [
        "- The following is an example of applying RoPE to the `q` and `k` tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "pTRcXI_o3e1D"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "batch_size = 2\n",
        "context_len = 5\n",
        "num_heads = 4\n",
        "head_dim = 16\n",
        "\n",
        "# Instantiate RoPE parameters\n",
        "cos, sin = precompute_rope_params(head_dim=head_dim, context_length=context_len)\n",
        "\n",
        "# Dummy query and key tensors\n",
        "torch.manual_seed(123)\n",
        "queries = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
        "keys = torch.randn(batch_size, num_heads, context_len, head_dim)\n",
        "\n",
        "# Apply rotary position embeddings\n",
        "queries_rot = compute_rope(queries, cos, sin)\n",
        "keys_rot = compute_rope(keys, cos, sin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TskUzBBw3e1D"
      },
      "source": [
        "&nbsp;\n",
        "## 1.5 Add RoPE to MultiHeadAttention module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2dzXAyk3e1D"
      },
      "source": [
        "- It's important to note that GPT applies the positional embeddings to the inputs, whereas Llama applies rotations to the query and key vectors in the self-attention mechanism itself\n",
        "- Here, we modify the `MultiHeadAttention` class with the appropriate RoPE code\n",
        "- In addition, we remove the `qkv_bias` option and hardcode the `bias=False` setting\n",
        "- Also, we add a dtype setting to be able to instantiate the model with a lower precision later\n",
        " - Tip: since the `TransformerBlock`s (in the next section) are repeated exactly, we could simplify the code and only initialize the buffers once instead for each `MultiHeadAttention` module; however, we add the precomputed RoPE parameters to the `MultiHeadAttention` class so that it can function as a standalone module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "4JAtYtz13e1D"
      },
      "outputs": [],
      "source": [
        "#####################################\n",
        "# MultiHeadAttention Module\n",
        "#####################################\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, num_heads, dtype=None):  # ,dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by n_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        ################################### NEW ###################################\n",
        "        # Set bias=False and dtype=dtype for all linear layers below\n",
        "        ###########################################################################\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=False, dtype=dtype)\n",
        "        self.out_proj = nn.Linear(d_out, d_out, bias=False, dtype=dtype)  # Linear layer to combine head outputs\n",
        "        # self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "        ################################### NEW ###################################\n",
        "        cos, sin = precompute_rope_params(head_dim=self.head_dim, context_length=context_length)\n",
        "        self.register_buffer(\"cos\", cos)\n",
        "        self.register_buffer(\"sin\", sin)\n",
        "        ###########################################################################\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        ################################### NEW ###################################\n",
        "        keys = compute_rope(keys, self.cos, self.sin)\n",
        "        queries = compute_rope(queries, self.cos, self.sin)\n",
        "        ###########################################################################\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        # attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.reshape(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9eAGEXN3e1E"
      },
      "source": [
        "- Below is an example using the `MultiHeadAttention` module on an example input:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "OaIoXj8n3e1E"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "batch_size = 1\n",
        "context_len = 100\n",
        "max_context_len = 4096\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "\n",
        "\n",
        "example_batch = torch.randn((batch_size, context_len, embed_dim))\n",
        "\n",
        "mha = MultiHeadAttention(\n",
        "    d_in=embed_dim,\n",
        "    d_out=embed_dim,\n",
        "    context_length=max_context_len,\n",
        "    num_heads=num_heads\n",
        ")\n",
        "\n",
        "mha(example_batch)\n",
        "\n",
        "del mha  # delete to free up memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY_w2_vq3e1F"
      },
      "source": [
        "&nbsp;\n",
        "## 1.6 Update the TransformerBlock module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbPkiGKF3e1F"
      },
      "source": [
        "- At this stage, most of the hard work is already done; we can now update the `TransformerBlock` to use the code we implemented above\n",
        "- This means we\n",
        " - replace LayerNorm with RMSNorm\n",
        " - remove dropout\n",
        " - remove the `qkv_bias` setting\n",
        " - add the `dtype` setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "YBJZlHDp3e1F"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dtype=cfg[\"dtype\"]  # NEW\n",
        "            # dropout=cfg[\"drop_rate\"],\n",
        "            # qkv_bias=cfg[\"qkv_bias\"]\n",
        "        )\n",
        "        self.ff = FeedForward(cfg)\n",
        "\n",
        "        ################################### NEW ###################################\n",
        "        # self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        # self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n",
        "        ###########################################################################\n",
        "\n",
        "        # self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)   # Shape [batch_size, num_tokens, emb_size]\n",
        "        # x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed-forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)\n",
        "        # x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCmTm_1D3e1F"
      },
      "source": [
        "&nbsp;\n",
        "## 1.7 Update the model class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHlFsehf3e1F"
      },
      "source": [
        "- the `TransformerBlock` is a repeated block within the main model\n",
        "- Our Llama model is almost complete; we just have to update the model code surrounding the `TransformerBlock`\n",
        "- This means we\n",
        "  - remove absolute positional embeddings since we have RoPE embeddings now\n",
        "  - replace LayerNorm with RMSNorm\n",
        "  - remove dropout\n",
        "  - add the dtype setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "FRbyTK9q3e1G"
      },
      "outputs": [],
      "source": [
        "# class GPTModel(nn.Module):\n",
        "class Llama2Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"])\n",
        "        # self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        ################################### NEW ###################################\n",
        "        # self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n",
        "        ###########################################################################\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        # batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        # pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds  # + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        # x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYv7qjLJ3e1G"
      },
      "source": [
        "# 2. Initialize model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xakWGuiQ3e1H"
      },
      "source": [
        "we used the following config file to specify the 124M-parameter GPT model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "f2uXNHSa3e1H"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 768,          # Embedding dimension\n",
        "    \"n_heads\": 12,           # Number of attention heads\n",
        "    \"n_layers\": 12,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zab5KxeQ3e1H"
      },
      "source": [
        "For reference, the 1.5B parameter GPT model config is shown below as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "TEgb5KvK3e1M"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_1558M = {\n",
        "    \"vocab_size\": 50257,     # Vocabulary size\n",
        "    \"context_length\": 1024,  # Context length\n",
        "    \"emb_dim\": 1600,         # Embedding dimension\n",
        "    \"n_heads\": 25,           # Number of attention heads\n",
        "    \"n_layers\": 48,          # Number of layers\n",
        "    \"drop_rate\": 0.1,        # Dropout rate\n",
        "    \"qkv_bias\": False        # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCGXrV0W3e1N"
      },
      "source": [
        "Similarly, we can define a Llama 2 config file for the 7B model (we ignore the other larger models for simplicity here):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<warning> **Please Note due to Google Colab , the num_heads and num_layers are reduced from originally 32 to 16** </warning>"
      ],
      "metadata": {
        "id": "4XIoidM_9uDS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "fJxysD3v3e1N"
      },
      "outputs": [],
      "source": [
        "LLAMA2_CONFIG_7B = {\n",
        "    \"vocab_size\": 32000,     # Vocabulary size\n",
        "    \"context_length\": 4096,  # Context length\n",
        "    \"emb_dim\": 4096,         # Embedding dimension\n",
        "    \"n_heads\": 16,           # Number of attention heads  # reduced n_heads from 32 t0 16 for less memory\n",
        "    \"n_layers\": 16,          # Number of layers # reduced n_layers from 32 t0 16 for less memory\n",
        "    \"hidden_dim\": 11008,     # NEW: Size of the intermediate dimension in FeedForward\n",
        "    \"dtype\": torch.bfloat16  # NEW: Lower-precision dtype to reduce memory usage\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bNdd30f3e1N"
      },
      "source": [
        "Using these settings, we can now initialize a Llama 2 7B model (note that this requires ~26 GB of memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "YtjdDmhU3e1N"
      },
      "outputs": [],
      "source": [
        "model = Llama2Model(LLAMA2_CONFIG_7B)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPs1GUOR3e1N",
        "outputId": "624d1411-014c-42c7-9bae-ab9f42fab507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 3,500,281,856\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gfn6zs03e1N"
      },
      "source": [
        "Additionally, we can calculate the memory requirements for this model using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YBggzqJ3e1N",
        "outputId": "92b46286-5339-4d2f-a927-531084a9806e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "float32 (PyTorch default): 27.20 GB\n",
            "bfloat16: 13.60 GB\n"
          ]
        }
      ],
      "source": [
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0\n",
        "    total_grads = 0\n",
        "    for param in model.parameters():\n",
        "        # Calculate total number of elements per parameter\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        # Check if gradients are stored for this parameter\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    # Calculate buffer size (non-parameters that require memory)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "    # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "    # We assume parameters and gradients are stored in the same type as input dtype\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "    # Convert bytes to gigabytes\n",
        "    total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    return total_memory_gb\n",
        "\n",
        "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmuAr24S3e1O"
      },
      "source": [
        "Lastly, we can also transfer the model to an NVIDIA or Apple Silicon GPU if applicable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3XRqR3bI3e1O"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9A-Na_Md3e1O"
      },
      "source": [
        "# 3. Load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4RHdYdm3e1O"
      },
      "source": [
        "1. In this section, we are going to load the tokenizer for the model\n",
        "2. Llama 2 uses Google's SentencePiece tokenizer instead of OpenAI's Tiktoken (but Llama 3 uses Tiktoken)\n",
        "3. Meta AI shared the original Llama 2 model weights and tokenizer vocabulary on the Hugging Face Hub\n",
        "4. We will download the tokenizer vocabulary from the Hub and load it into SentencePiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKtauhax3e1P",
        "outputId": "643bd3b6-b659-48e0-8127-86916ae1039e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4g0Iv0ic3e1P"
      },
      "source": [
        "- Please note that Meta AI requires that you accept the Llama 2 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the [meta-llama/Llama-2-7b](https://huggingface.co/meta-llama/Llama-2-7b) repository to accept the terms\n",
        "- Next, you will need to create an access token; to generate an access token with READ permissions, click on the profile picture in the upper right and click on \"Settings\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "access_token = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "ktd47NPn8Ucs"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DBvJjoMF3e1P"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import json\n",
        "\n",
        "#if running from local setup HF TOKEN in config.\n",
        "#with open(\"config.json\", \"r\") as config_file:\n",
        "#    config = json.load(config_file)\n",
        "#    access_token = config[\"HF_ACCESS_TOKEN\"]\n",
        "\n",
        "login(token=access_token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WTzizF53e1Q"
      },
      "source": [
        "After login via the access token, which is necessary to verify that we accepted the Llama 2 licensing terms, we can now download the tokenizer vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "lasZNPTH3e1Q"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "tokenizer_file = hf_hub_download(\n",
        "    repo_id=\"meta-llama/Llama-2-7b\",\n",
        "    filename=\"tokenizer.model\",\n",
        "    local_dir=\"Llama-2-7b\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWGjGeDy3e1Q"
      },
      "source": [
        "To provide a more familiar interface for the tokenizer, we define a small LlamaTokenizer wrapper class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "Lb2wq4pb3e1Q"
      },
      "outputs": [],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "\n",
        "class LlamaTokenizer:\n",
        "    def __init__(self, tokenizer_file):\n",
        "        sp = spm.SentencePieceProcessor()\n",
        "        sp.load(tokenizer_file)\n",
        "        self.tokenizer = sp\n",
        "\n",
        "    def encode(self, text):\n",
        "        return self.tokenizer.encode_as_ids(text)\n",
        "\n",
        "    def decode(self, ids):\n",
        "        return self.tokenizer.decode_pieces(ids)\n",
        "\n",
        "\n",
        "tokenizer = LlamaTokenizer(tokenizer_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MREgBp8w3e1R"
      },
      "source": [
        "## 3.1 Helper functions for generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "EwxCDEYq3e1R"
      },
      "outputs": [],
      "source": [
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text)\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "\n",
        "    # For-loop is the same as before: Get logits, and only focus on last time step\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # New: Filter logits with top_k sampling\n",
        "        if top_k is not None:\n",
        "            # Keep only top_k values\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1]\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float('-inf')).to(logits.device), logits)\n",
        "\n",
        "        # New: Apply temperature scaling\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply softmax to get probabilities\n",
        "            probs = torch.softmax(logits, dim=-1)  # (batch_size, context_len)\n",
        "\n",
        "            # Sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
        "\n",
        "        # Otherwise same as before: get idx of the vocab entry with the highest logits value\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # (batch_size, 1)\n",
        "\n",
        "        if idx_next == eos_id:  # Stop generating early if end-of-sequence token is encountered and eos_id is specified\n",
        "            break\n",
        "\n",
        "        # Same as before: append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch_size, num_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNXDah3S3e1R"
      },
      "source": [
        "We can now use the generate function to have the Llama 2 model generate new text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSPhjiOs3e1R"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort moves\", tokenizer).to(device),\n",
        "    max_new_tokens=30,\n",
        "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Of course, as we can see above, the text is nonsensical since we haven't trained the Llama 2 model yet\n",
        "- In the next section, instead of training it ourselves, which would cost tens to hundreds of thousands of dollars, we load the pretrained weights from Meta AI"
      ],
      "metadata": {
        "id": "Q2_7_9yp-QnG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Load pretrained weights"
      ],
      "metadata": {
        "id": "b2inCPjLBSMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We are loading the [\"meta-llama/Llama-2-7b\"](https://huggingface.co/meta-llama/Llama-2-7b) base model below, which is a simple text completion model before finetuning"
      ],
      "metadata": {
        "id": "kyNVfK-gBcxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights_file = hf_hub_download(\n",
        "   repo_id=\"meta-llama/Llama-2-7b\",\n",
        "   filename=\"consolidated.00.pth\",\n",
        "   local_dir=\"Llama-2-7b\"\n",
        ")"
      ],
      "metadata": {
        "id": "2r-YQUCoBd9V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.load(weights_file, weights_only=True,map_location=\"cuda\")"
      ],
      "metadata": {
        "id": "nayoIN58Bkzi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(weights.keys())[:15]"
      ],
      "metadata": {
        "id": "bX4zvWKSBmyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following function, loads the pretrained weights into our Llama 2 model:"
      ],
      "metadata": {
        "id": "_Gvq77DXClBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign(left, right):\n",
        "    if left.shape != right.shape:\n",
        "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
        "\n",
        "    if isinstance(right, torch.Tensor):\n",
        "        return torch.nn.Parameter(right.clone().detach())\n",
        "    else:\n",
        "        return torch.nn.Parameter(torch.tensor(right))\n",
        "\n",
        "\n",
        "def load_weights_into_llama(model, param_config, params):\n",
        "    model.tok_emb.weight = assign(model.tok_emb.weight, params[\"tok_embeddings.weight\"])\n",
        "\n",
        "    for l in range(param_config[\"n_layers\"]):\n",
        "\n",
        "        # Load attention weights\n",
        "        model.trf_blocks[l].att.W_query.weight = assign(\n",
        "            model.trf_blocks[l].att.W_query.weight,\n",
        "            params[f\"layers.{l}.attention.wq.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_key.weight = assign(\n",
        "            model.trf_blocks[l].att.W_key.weight,\n",
        "            params[f\"layers.{l}.attention.wk.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].att.W_value.weight = assign(\n",
        "            model.trf_blocks[l].att.W_value.weight,\n",
        "            params[f\"layers.{l}.attention.wv.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].att.out_proj.weight = assign(\n",
        "            model.trf_blocks[l].att.out_proj.weight,\n",
        "            params[f\"layers.{l}.attention.wo.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].norm1.weight = assign(\n",
        "            model.trf_blocks[l].norm1.weight,\n",
        "            params[f\"layers.{l}.attention_norm.weight\"]\n",
        "        )\n",
        "\n",
        "        # Load FeedForward weights\n",
        "        model.trf_blocks[l].ff.fc1.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc1.weight,\n",
        "            params[f\"layers.{l}.feed_forward.w1.weight\"]\n",
        "        )\n",
        "        # For some reason w2 and w3 are provided in the wrong order in the weights file\n",
        "        model.trf_blocks[l].ff.fc2.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc2.weight,\n",
        "            params[f\"layers.{l}.feed_forward.w3.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].ff.fc3.weight = assign(\n",
        "            model.trf_blocks[l].ff.fc3.weight,\n",
        "            params[f\"layers.{l}.feed_forward.w2.weight\"]\n",
        "        )\n",
        "        model.trf_blocks[l].norm2.weight = assign(\n",
        "            model.trf_blocks[l].norm2.weight,\n",
        "            params[f\"layers.{l}.ffn_norm.weight\"]\n",
        "        )\n",
        "\n",
        "    # Load output layer weights\n",
        "    model.final_norm.weight = assign(model.final_norm.weight, params[\"norm.weight\"])\n",
        "    model.out_head.weight = assign(model.out_head.weight, params[\"output.weight\"])\n",
        "\n",
        "\n",
        "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
        "model.to(device);"
      ],
      "metadata": {
        "id": "Sj1O0Uy0IZpy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we are ready to use the model for text generation"
      ],
      "metadata": {
        "id": "XRPu97TPIth8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"Every effort\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "Eur9Tp3_IvF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Using the instruction-finetuned model"
      ],
      "metadata": {
        "id": "VrApAjUTIzUy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned earlier, above we used the pretrained base model; if you want to use a model capable of following instructions, use the \"meta-llama/Llama-2-7b-chat\" model instead, as shown below"
      ],
      "metadata": {
        "id": "OPLSTcSCI5I3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model  # to free up memory\n",
        "\n",
        "weights_file = hf_hub_download(\n",
        "   repo_id=\"meta-llama/Llama-2-7b-chat\",\n",
        "   filename=\"consolidated.00.pth\",\n",
        "   local_dir=\"Llama-2-7b-chat\"\n",
        ")\n",
        "\n",
        "model = Llama2Model(LLAMA2_CONFIG_7B)\n",
        "load_weights_into_llama(model, LLAMA2_CONFIG_7B, weights)\n",
        "model.to(device);\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "token_ids = generate(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(\"What do llamas eat?\", tokenizer).to(device),\n",
        "    max_new_tokens=25,\n",
        "    context_size=LLAMA2_CONFIG_7B[\"context_length\"],\n",
        "    top_k=1,\n",
        "    temperature=0.\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "id": "SADGmufvI52z"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}